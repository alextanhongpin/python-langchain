{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551a7f1c-957d-4dbe-b58e-a1c63e639a34",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The types of models that are used by LangChain\n",
    "\n",
    "- __LLMs__: models that takes a text string as input, and return a text string as output\n",
    "- __Chat Models__: these models are backed by a language model, but their APIs are more structured. These models takes a list of Chat Messages as input, and return a Chat Message.\n",
    "- __Text Embedding Models__: these models takes text as input and returns a list of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3330f1a-49e3-4fc4-9c2b-e9f55fc4fc1c",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "LangChain provides a standard interface through which you can interact with a variety of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ff76a-e857-447c-b371-274defeb56ad",
   "metadata": {},
   "source": [
    "We can use LLM to generate a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622fa318-cb84-4c91-9466-b79804e7493d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side!\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "print(llm(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517628a-bc39-4411-89dc-0af751001229",
   "metadata": {},
   "source": [
    "We can also send a batch of messages with the `llm.generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1807dc-9d9b-4490-8bd8-a19af0469ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe AI of tomorrow\\n\\nWill be composed of machines\\n\\nThat can think, and feel\\n\\nAnd love, and want, and love\\n\\nAs if they were really people', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\n\\n\\nThe future is art,\\nand you will be its most Needy Kitten\\nYou will need all of your skill\\nto make it look so difficult\\nBut you won't, because you will be able to get what you want\", generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 127, 'total_tokens': 137, 'prompt_tokens': 10}, 'model_name': 'text-ada-001'}\n"
     ]
    }
   ],
   "source": [
    "llm_result = llm.generate([\"tell me a joke\", \"recite a poem about AI\"])\n",
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51986a06-f4f5-4337-a9c4-66b9feaea3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba399e1-4853-44f0-b3eb-f833d8b26fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text='\\n\\nThe AI of tomorrow\\n\\nWill be composed of machines\\n\\nThat can think, and feel\\n\\nAnd love, and want, and love\\n\\nAs if they were really people', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\n\\n\\nThe future is art,\\nand you will be its most Needy Kitten\\nYou will need all of your skill\\nto make it look so difficult\\nBut you won't, because you will be able to get what you want\", generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "print(llm_result.generations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4921cb4-a6b8-4ab3-970f-573201ed93c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Some provider provides specific information. This information is NOT standardized across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa0748e9-31ab-423a-8752-2981c28ed472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 127,\n",
       "  'total_tokens': 137,\n",
       "  'prompt_tokens': 10},\n",
       " 'model_name': 'text-ada-001'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8424fd-22f7-41ba-a438-de3c9d285259",
   "metadata": {},
   "source": [
    "You can also estimate how many tokens a piece of text will be in that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3b1d7b-6a45-4721-880d-74ee338211ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(\"what a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243492a1-f454-4f49-bac7-8732a3c187b3",
   "metadata": {},
   "source": [
    "### Using async API for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fb0c3d9-11d1-4afd-8340-8728bee5306e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    for _ in range(5):\n",
    "        resp = llm.generate([\"hello, how are you?\"])\n",
    "        print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"hello, how are you?\"])\n",
    "    print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(5)]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f939b8d2-5ba3-48e5-b849-55398c31c767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks. How about you?\n",
      "\n",
      "\n",
      "Hi there, I'm doing well. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "I'm doing well, thank you. How are you?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4431627320000189"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "await generate_concurrently()\n",
    "elapsed = time.perf_counter() - start\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d74c66aa-b268-4f13-ba7f-7ab52a39679b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you for asking! How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you for asking! How about you?\n",
      "\n",
      "\n",
      "I'm doing great, thank you for asking. How are you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about yourself?\n",
      "\n",
      "\n",
      "I'm doing great, thanks for asking! How about you?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.626748034000002"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - start\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0d3db-3e0a-4f79-9c20-e33cb646badb",
   "metadata": {},
   "source": [
    "### Writing a custom LLM wrapper\n",
    "\n",
    "Just extends the `LLM` class, and implement:\n",
    "\n",
    "- a `_call` method that takes in a string, some optional stop words, and returns a string\n",
    "- an `_identifying_params` (optional) property that is used to help with printing of this class. Should return a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dcc75a0-bcf8-49a2-a143-9573a54b413b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea0e2f97-eba8-467e-b15b-e1de4b346ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying params.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82ecfd57-18c1-4157-acec-37fce927f6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = CustomLLM(n=10)\n",
    "llm(\"This is a foobar string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d41457-a84f-4610-8ac9-cecdba096d2a",
   "metadata": {},
   "source": [
    "Printing the LLM will print the `_identifying params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4f3a776-ce8c-4cf7-8add-7154176bc21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78574836-4e4b-474a-ba88-fd33e7218db6",
   "metadata": {},
   "source": [
    "### Using the fake LLM\n",
    "\n",
    "Why should you use the fake LLM?\n",
    "- mock out calls to LLM \n",
    "- simulate what would happen if the LLM responded in a certain way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cd6486b-6a14-4bcc-b4e5-6f88ec54f6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.llms.fake import FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "414034b2-7fb6-467b-9dd6-f7960b502beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeListLLM(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x11883ccd0>, responses=['Action: Python REPL\\nAction Input: print(2 + 2)', 'Final Answer: 4'], i=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools([\"python_repl\"])\n",
    "responses = [\"Action: Python REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65ce2416-a291-44dc-a5a8-b41bd8b264fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0182c7-bd5b-407e-be7c-a9d75ef2a548",
   "metadata": {},
   "source": [
    "### Caching LLM calls\n",
    "\n",
    "You can cache the results of individual LLM calls. The types of cache available:\n",
    "\n",
    "- in memory cache\n",
    "- sqlite cache\n",
    "- redis cache\n",
    "- [GPTCache](https://github.com/zilliztech/GPTCache)\n",
    "- SQLAlchemy Cache\n",
    "\n",
    "\n",
    "You can also turn off caching for specific LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3954b804-f0ce-4317-9c20-3ea1b55b0a09",
   "metadata": {},
   "source": [
    "### Saving and loading LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53a67884-062b-446b-9885-fc39daa71b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "llm.save(\"llm.json\")\n",
    "llm.save(\"llm.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e94f2091-0e5d-4c69-b954-6fcf78d24e88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_name\": \"text-ada-001\",\n",
      "    \"temperature\": 0.7,\n",
      "    \"max_tokens\": 256,\n",
      "    \"top_p\": 1,\n",
      "    \"frequency_penalty\": 0,\n",
      "    \"presence_penalty\": 0,\n",
      "    \"n\": 2,\n",
      "    \"best_of\": 2,\n",
      "    \"request_timeout\": null,\n",
      "    \"logit_bias\": {},\n",
      "    \"_type\": \"openai\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat llm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f76621a9-edfa-4fdc-983d-f61c36841b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_type: openai\n",
      "best_of: 2\n",
      "frequency_penalty: 0\n",
      "logit_bias: {}\n",
      "max_tokens: 256\n",
      "model_name: text-ada-001\n",
      "n: 2\n",
      "presence_penalty: 0\n",
      "request_timeout: null\n",
      "temperature: 0.7\n",
      "top_p: 1\n"
     ]
    }
   ],
   "source": [
    "!cat llm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5cc258f-b5ac-48ba-8128-cfc72bb64940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x11883ccd0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-ada-001', temperature=0.7, max_tokens=256, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, n=2, best_of=2, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_llm(\"llm.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a69c7a29-66cb-473b-9bd1-b8ce9a6b1d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x11883ccd0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-ada-001', temperature=0.7, max_tokens=256, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, n=2, best_of=2, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_llm(\"llm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf64718-51be-4300-8615-570f196f18dc",
   "metadata": {},
   "source": [
    "### Tracking Token Usage\n",
    "\n",
    "Currently it is only implemented for OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2380aff-ba6c-4bc9-806c-4a95ac22c9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side!\n",
      "Total Tokens: 42\n",
      "Prompt Tokens: 4\n",
      "Completion Tokens: 38\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $1.6800000000000002e-05\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Cheapest model: https://platform.openai.com/docs/models/overview\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"tell me a joke\")\n",
    "    print(result)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Successful Requests: {cb.successful_requests}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
